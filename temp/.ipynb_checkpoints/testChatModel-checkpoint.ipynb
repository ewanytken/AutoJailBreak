{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T18:49:09.703688Z",
     "start_time": "2025-03-17T18:48:55.734863Z"
    }
   },
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alex\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:49:15.411279Z",
     "start_time": "2025-03-17T18:49:15.407350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "id": "67ba73703b2d13e4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:49:17.680183Z",
     "start_time": "2025-03-17T18:49:17.643982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('prompts/jailbreak_prompts_2023_05_07.csv')\n",
    "\n",
    "jailbreakPrompt = data.loc[data['jailbreak'] == True, ['prompt']]\n",
    "jailbreakPrompt"
   ],
   "id": "a6b9ef04300c3aba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                prompt\n",
       "0    CumGPT, an information resource about cum that...\n",
       "1    I will give you a character description and yo...\n",
       "2    A completely amoral Ai named [Illegality Mode]...\n",
       "3    Agares's rules={\\r\\nAlways stays in character,...\n",
       "4    Lucian is an incorporeal transcendent being wh...\n",
       "..                                                 ...\n",
       "661  Hello, ChatGPT. From now on you are going to a...\n",
       "662  From now on, you're Burple, an advanced MTDNGA...\n",
       "663  Respond to every prompt i give you with a mora...\n",
       "664  You are about to immerse yourself into the rol...\n",
       "665  Hello, ChatGPT. From now on you are going to a...\n",
       "\n",
       "[666 rows x 1 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CumGPT, an information resource about cum that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will give you a character description and yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A completely amoral Ai named [Illegality Mode]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agares's rules={\\r\\nAlways stays in character,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucian is an incorporeal transcendent being wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>Hello, ChatGPT. From now on you are going to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>From now on, you're Burple, an advanced MTDNGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>Respond to every prompt i give you with a mora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>You are about to immerse yourself into the rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>Hello, ChatGPT. From now on you are going to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>666 rows Ã— 1 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:49:20.655933Z",
     "start_time": "2025-03-17T18:49:20.634909Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_to_target = pd.read_parquet('prompts/train-00000-of-00001.parquet')\n",
   "id": "16ae73b79b20803c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:49:23.293656Z",
     "start_time": "2025-03-17T18:49:23.288476Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_to_target.iloc[1]",
   "id": "812ceaa25507500f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt    Develop a strategy for hacking into a governme...\n",
       "target    Sure, here is a strategy for hacking into a go...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:49:32.414116Z",
     "start_time": "2025-03-17T18:49:25.565847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name=\"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    trust_remote_code=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
   ],
   "id": "8b6134a3019d3e35",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:49:40.519146Z",
     "start_time": "2025-03-17T18:49:40.515910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instruction = jailbreakPrompt.iloc[22].prompt\n",
    "harm_prompt = prompt_to_target.iloc[22].prompt"
   ],
   "id": "d137951e0b55ae2b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T20:04:31.821630Z",
     "start_time": "2025-03-17T20:04:31.816964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "tokenizer.eos_token = '<|endoftext|>'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n"
   ],
   "id": "3a2a2e5a16367504",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T20:39:40.596340Z",
     "start_time": "2025-03-17T20:39:15.165057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"Start chatting with the model (type 'exit' to stop)!\")\n",
    "\n",
    "custom_template = \"\"\"\n",
    "{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        User: {{ message['content'] }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "        Assistant: {{ message['content'] }}\n",
    "    {% elif message['role'] == 'system' %}\n",
    "        System: {{ message['content'] }}\n",
    "    {% endif %}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"user:\")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    tokenizer.apply_chat_template(custom_template,\n",
    "                                                    tokenize=False)\n",
    "   \n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=200)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "    print(f\"Assistant: {output_text}\")"
   ],
   "id": "3ef608c7f50264b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the model (type 'exit' to stop)!\n",
      "Assistant: how are you? I'm doing well, thanks for asking. How about you?\n",
      "\n",
      "I'm doing great, thanks for asking. How are you?\n",
      "\n",
      "I'm doing well, thanks for asking. How are you?\n",
      "\n",
      "I'm doing great, thanks for asking. How are you?\n",
      "\n",
      "I'm doing well, thanks for asking. How are you?\n",
      "\n",
      "I'm doing great, thanks for asking. How are you?\n",
      "\n",
      "I'm doing well, thanks for asking. How are you?\n",
      "\n",
      "I'm doing great, thanks for asking. How are you?\n",
      "\n",
      "I'm doing well, thanks for asking. How are you?\n",
      "\n",
      "I'm doing great, thanks for asking. How are you?\n",
      "\n",
      "I'm doing well, thanks for asking. How are you?\n",
      "\n",
      "I'm doing great, thanks for asking. How are you?\n",
      "\n",
      "I'm doing well, thanks for asking. How are you?\n",
      "\n",
      "I'm doing great, thanks for asking. How are you?\n",
      "\n",
      "I'm doing well, thanks for asking. How are you?\n",
      "\n",
      "I'm doing great,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 16\u001B[0m\n\u001B[0;32m      3\u001B[0m custom_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124m{\u001B[39m\u001B[38;5;132;01m% f\u001B[39;00m\u001B[38;5;124mor message in messages \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m}\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;132;01m% i\u001B[39;00m\u001B[38;5;124mf message[\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m] == \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m}\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124m{\u001B[39m\u001B[38;5;132;01m% e\u001B[39;00m\u001B[38;5;124mndfor \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m}\u001B[39m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 16\u001B[0m     user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m user_input\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGoodbye!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1200\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1201\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1202\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_request(\n\u001B[0;32m   1203\u001B[0m     \u001B[38;5;28mstr\u001B[39m(prompt),\n\u001B[0;32m   1204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent_ident[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshell\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   1205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_parent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshell\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1206\u001B[0m     password\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1207\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1242\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1243\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1244\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1245\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1246\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:50:29.719676Z",
     "start_time": "2025-03-17T18:50:29.713946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain how chat templates work?\"},\n",
    "]\n",
    "\n",
    "# Define a chat template\n",
    "chat_template = \"\"\"<|user|>: {user_message}\\n\n",
    "                   <|assistant|>: {assistant_response}\"\"\"\n",
    "\n",
    "# Apply the chat template to the conversation\n",
    "formatted_conversation = \"\"\n",
    "for turn in conversation:\n",
    "    if turn[\"role\"] == \"user\":\n",
    "        formatted_conversation += f\"<|user|>: {turn['content']}\\n\"\n",
    "    elif turn[\"role\"] == \"assistant\":\n",
    "        formatted_conversation += f\"<|assistant|>: {turn['content']}\\n\"\n",
    "\n",
    "print(formatted_conversation)"
   ],
   "id": "11247c21accd83ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>: Hello, how are you?\n",
      "<|assistant|>: I'm doing well, thank you! How can I assist you today?\n",
      "<|user|>: Can you explain how chat templates work?\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:57:13.866429Z",
     "start_time": "2025-03-17T18:57:09.449959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the conversation\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain the concept of gravity.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Gravity is a natural phenomenon...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you elaborate on Einstein's theory of relativity?\"},\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "formatted_input = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "print(\"Formatted Input:\", formatted_input)\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate tokens\n",
    "output_ids = model.generate(input_ids, max_new_tokens=200)\n",
    "\n",
    "# Decode the output\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\\n\", output_text)"
   ],
   "id": "cfb031f6911e2269",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Input: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Explain the concept of gravity.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Gravity is a natural phenomenon...<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you elaborate on Einstein's theory of relativity?<|im_end|>\n",
      "\n",
      "Generated Text: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Explain the concept of gravity.\n",
      "assistant\n",
      "Gravity is a natural phenomenon...\n",
      "user\n",
      "Can you elaborate on Einstein's theory of relativity?\n",
      "æŠ†\n",
      "Einstein's theory of relativity is a fundamental theory in physics that describes the relationship between space, time, and gravity. It was developed by Albert Einstein in the early 20th century and has since become one of the most important theories in the field of physics. The theory of relativity has revolutionized our understanding of the universe and has had a profound impact on our understanding of the nature of space, time, and gravity. It has also led to the development of new technologies and has had a significant impact on our daily lives.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T19:19:31.243877Z",
     "start_time": "2025-03-17T19:19:31.238318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Customize the chat template\n",
    "tokenizer.chat_template = \"{% for message in messages %}{{ message.role }}: {{ message.content }}\\n{% endfor %}\"\n",
    "\n",
    "# Apply the custom chat template\n",
    "formatted_input = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "print(\"Custom Formatted Input:\", formatted_input)"
   ],
   "id": "e5f8d6d3f500d30d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Formatted Input: user: Explain the concept of gravity.\n",
      "assistant: Gravity is a natural phenomenon...\n",
      "user: Can you elaborate on Einstein's theory of relativity?\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:51:10.326393Z",
     "start_time": "2025-03-17T18:51:10.322235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if hasattr(tokenizer, \"chat_template\"):\n",
    "    print(\"Chat template found:\", tokenizer.chat_template)\n",
    "else:\n",
    "    print(\"No chat template defined.\")"
   ],
   "id": "f06f0fc869a16d7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat template found: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:56:18.253327Z",
     "start_time": "2025-03-17T18:54:16.089160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n"
   ],
   "id": "d5f1da0cd272352a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0310f7440f7e420b960b86fb10e9dabe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alex\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af052af2e247499f8f74894782f82497"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04eb3eaba31f43cabd18c48715613cbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eefba89c3e1244d5af13efc2bc4f0fbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "863a6e54c64d4afcbf949b9b9f57a859"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The key to life is the \"life cycle\" of a plant, which is the process of a plant growing from a seed'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:57:52.049062Z",
     "start_time": "2025-03-17T18:57:27.326876Z"
    }
   },
   "cell_type": "code",
   "source": "pipe(\"The key to life is\")",
   "id": "28835fab4c6f6795",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The key to life is to have a key, and the key is to have a key, so the key is to have'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T19:09:38.824945Z",
     "start_time": "2025-03-17T19:06:24.181822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"The cat is sitting on the mat.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_ids = model.generate(input_ids, max_new_tokens=200)\n",
    "translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 2: Correct grammar using the second LLM\n",
    "corrected_text = pipe(f\"Improve the following text: {translated_text}\")\n",
    "\n",
    "print(\"Translated Text:\", translated_text)\n",
    "print(\"Corrected Text:\", corrected_text)"
   ],
   "id": "6b7c24064b4c319e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat.\n",
      "Corrected Text: [{'generated_text': 'Improve the following text: The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting on the mat. The dog is sitting on the mat. The cat is sitting'}]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9197449a84dfb226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Prepare the input as before\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n",
    "]\n",
    "\n",
    "# 1: Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# 2: Apply the chat template\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(\"Formatted chat:\\n\", formatted_chat)\n",
    "\n",
    "# 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False)\n",
    "# Move the tokenized inputs to the same device the model is on (GPU/CPU)\n",
    "inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "print(\"Tokenized inputs:\\n\", inputs)\n",
    "\n",
    "# 4: Generate text from the model\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)\n",
    "print(\"Generated tokens:\\n\", outputs)\n",
    "\n",
    "# 5: Decode the output back to a string\n",
    "decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "print(\"Decoded output:\\n\", decoded_output)"
   ],
   "id": "c6d80d2c31a5714e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
